{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sakshi18013/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from os.path import join\n",
    "\n",
    "# import utils\n",
    "from glimpse_attention_model import GlimpseAttentionModel\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "def _retype(y_prob, y):\n",
    "    if not isinstance(y, (collections.Sequence, np.ndarray)):\n",
    "        y_prob = [y_prob]\n",
    "        y = [y]\n",
    "    y_prob = np.array(y_prob)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return y_prob, y\n",
    "\n",
    "\n",
    "def _binarize(y, n_classes=None):\n",
    "    return label_binarize(y, classes=range(n_classes))\n",
    "\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def mapk(y_prob, y, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    predicted = [np.argsort(p_)[-k:][::-1] for p_ in y_prob]\n",
    "    actual = [[y_] for y_ in y]\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "\n",
    "def mean_rank(y_prob, y):\n",
    "    ranks = []\n",
    "    n_classes = y_prob.shape[1]\n",
    "    for p_, y_ in zip(y_prob, y):\n",
    "        ranks += [n_classes - rankdata(p_, method='max')[y_]]\n",
    "\n",
    "    return sum(ranks) / float(len(ranks))\n",
    "\n",
    "\n",
    "def hits_k(y_prob, y, k=10):\n",
    "    acc = []\n",
    "    for p_, y_ in zip(y_prob, y):\n",
    "        top_k = p_.argsort()[-k:][::-1]\n",
    "        acc += [1. if y_ in top_k else 0.]\n",
    "    return sum(acc) / len(acc)\n",
    "\n",
    "\n",
    "# def roc_auc(y_prob, y):\n",
    "#     y = _binarize(y, n_classes=y_prob.shape[1])\n",
    "#     fpr, tpr, _ = roc_curve(y.ravel(), y_prob.ravel())\n",
    "#     return auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# def log_prob(y_prob, y):\n",
    "#     scores = []\n",
    "#     for p_, y_ in zip(y_prob, y):\n",
    "#         assert abs(np.sum(p_) - 1) < 1e-8\n",
    "#         scores += [-math.log(p_[y_]) + 1e-8]\n",
    "#         print p_, y_\n",
    "\n",
    "#     return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "def portfolio(y_prob, y, k_list=None):\n",
    "    y_prob, y = _retype(y_prob, y)\n",
    "    # scores = {'auc': roc_auc(y_prob, y)}\n",
    "    # scores = {'mean-rank:': mean_rank(y_prob, y)}\n",
    "    scores = {}\n",
    "    for k in k_list:\n",
    "        scores['hits@' + str(k)] = hits_k(y_prob, y, k=k)\n",
    "        scores['map@' + str(k)] = mapk(y_prob, y, k=k)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_seen_nodes(data_path, seq_len):\n",
    "    seen_nodes = []\n",
    "    with open(join(data_path, 'train.txt'), 'r') as read_file:\n",
    "        for i, line in enumerate(read_file):\n",
    "            query, cascade = line.strip().split(' ', 1)\n",
    "            sequence = cascade.split(' ')[::3]\n",
    "            seen_nodes.extend(sequence)\n",
    "            \n",
    "    with open(join(data_path, 'test.txt'), 'r') as read_file:\n",
    "        for i, line in enumerate(read_file):\n",
    "            query, cascade = line.strip().split(' ', 1)\n",
    "            sequence = cascade.split(' ')[::3]\n",
    "            seen_nodes.extend(sequence)\n",
    "    seen_nodes = set(seen_nodes)\n",
    "    with open(join(data_path, 'seen_nodes.txt'), 'w+') as write_file:\n",
    "        for node in seen_nodes:\n",
    "            write_file.write(node + '\\n')\n",
    "#     print(len(seen_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def process_timestamps(timestamps):\n",
    "#     print(len(timestamps))\n",
    "    lst2 = []\n",
    "    for i in range(0, len(timestamps), 2):\n",
    "        lst2.append(timestamps[i] +\" \" +timestamps[i+1])\n",
    "#     print(lst2)\n",
    "    diff =[]\n",
    "    for t in range(0,len(lst2)-1):\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        timeA  = datetime.datetime.strptime(lst2[t+1], format)\n",
    "        timeB  = datetime.datetime.strptime(lst2[t], format)\n",
    "        \n",
    "        diff.append(((timeA-timeB).total_seconds()/60.0))\n",
    "    \n",
    "    \n",
    "#     arr = np.asarray(lst2)\n",
    "# #     diff = list(np.diff(arr))\n",
    "#     print(diff)\n",
    "#     diff = [d / 60.0 for d in diff]\n",
    "#     format = '%Y-%m-%d %H:%M:%S'\n",
    "#     timeA  = datetime.datetime.strptime(t1, format)\n",
    "#     timeB  = datetime.datetime.strptime(tweet_time[k2_int], format)\n",
    "#     ((timeA-timeB).total_seconds()/60.0)\n",
    "    for i in range(0, len(diff)-1):\n",
    "        if diff[i] == diff[i+1]:\n",
    "            for j in range(0, len(diff)):\n",
    "                diff[j] += 0.25\n",
    "#     print(len(diff))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(node_index)\n",
    "# load_instances('data/twitter','train',node_index,100,limit=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cascade(cascade, timestamps, testing=False):\n",
    "    size = len(cascade)\n",
    "    examples = []\n",
    "    for i, node in enumerate(cascade):\n",
    "        if i == size - 1 and not testing:\n",
    "            return examples\n",
    "        if i < size - 1 and testing:\n",
    "            continue\n",
    "        prefix_c = cascade[: i + 1]\n",
    "        prefix_t = timestamps[: i + 1]\n",
    "        # predecessors = set(network[node]) & set(prefix_c)\n",
    "        # others = set(prefix_c).difference(set(predecessors))\n",
    "\n",
    "        '''if i == 0:\n",
    "            times.extend([0.0])\n",
    "        else:\n",
    "            # print(i)\n",
    "            times.extend([(timestamps[i-1] - timestamps[i])])'''\n",
    "\n",
    "        if not testing:\n",
    "            label_n = cascade[i+1]\n",
    "            label_t = timestamps[i+1]\n",
    "        else:\n",
    "            label_n = None\n",
    "            label_t = None\n",
    "\n",
    "        example = {'sequence': prefix_c, 'time': prefix_t,\n",
    "                   'label_n': label_n, 'label_t': label_t}\n",
    "\n",
    "        if not testing:\n",
    "            examples.append(example)\n",
    "        else:\n",
    "            return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index=load_graph('data/twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(data_path):\n",
    "    node_file = join(data_path, 'seen_nodes.txt')\n",
    "    with open(node_file, 'r') as f:\n",
    "        seen_nodes = [int(x.strip()) for x in f]\n",
    "    \n",
    "\n",
    "    # builds node index\n",
    "    node_index = {v: i for i, v in enumerate(seen_nodes)}\n",
    "#     print(node_index)\n",
    "    return node_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0f21bcfd4885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'node_index' is not defined"
     ]
    }
   ],
   "source": [
    "print(node_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instances(data_path, file_type, node_index, seq_len, limit, ratio=1.0, testing=False):\n",
    "    print(len(node_index))\n",
    "    max_diff = 0\n",
    "    pkl_path = join(data_path, file_type + '.pkl')\n",
    "#     if isfile(pkl_path):\n",
    "#         instances = pickle.load(open(pkl_path, 'rb'))\n",
    "#     else:\n",
    "    file_name = join(data_path, file_type + '.txt')\n",
    "    instances = []\n",
    "    with open(file_name, 'r') as read_file:\n",
    "        for i, line in enumerate(read_file):\n",
    "            query, cascade = line.strip().split(' ', 1)\n",
    "            cascade_nodes = list(map(int, cascade.split(' ')[::3]))\n",
    "            cascade_times2 = list(cascade.split(' ')[2::3])\n",
    "            cascade_times1 = list(cascade.split(' ')[1::3])\n",
    "            cascade_times = (np.array([[i, j] for i, j in zip(cascade_times1, cascade_times2)]).ravel())\n",
    "\n",
    "            if seq_len is not None:\n",
    "                cascade_nodes = cascade_nodes[:seq_len+1]\n",
    "                cascade_times = cascade_times[:(seq_len*2)+2]\n",
    "#                 print(\"before\",len(cascade_nodes),len(cascade_times))\n",
    "                if len(cascade_nodes) == (len(cascade_times)/2):\n",
    "                    cascade_nodes.pop()\n",
    "                cascade_times = process_timestamps(cascade_times)\n",
    "#                 print(\"after\",len(cascade_nodes),len(cascade_times))\n",
    "                assert len(cascade_nodes) == len(cascade_times)\n",
    "                \n",
    "            cascade_nodes = [node_index[x] for x in cascade_nodes]\n",
    "            if not cascade_nodes or not cascade_times:\n",
    "                continue\n",
    "            max_diff = max(max_diff, max(cascade_times))\n",
    "            ins = process_cascade(cascade_nodes, cascade_times, testing)\n",
    "            instances.extend(ins)\n",
    "            \n",
    "            if limit is not None and i == limit:\n",
    "                break\n",
    "        # pickle.dump(instances, open(pkl_path, 'wb+'))\n",
    "    total_samples = len(instances)\n",
    "    indices = np.random.choice(total_samples, int(total_samples * ratio), replace=False)\n",
    "#     print(indices)\n",
    "    sampled_instances = [instances[i] for i in indices]\n",
    "#     print(sampled_instances)\n",
    "    return sampled_instances, max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data=pickle.load(open(\"/home/sakshi18013/Documents/codes/final_data_deep_diffuse_sorted_jamia.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:70]\n",
    "test_data = data[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twitter/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-312ab228329c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#         print(*data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter/train.txt'"
     ]
    }
   ],
   "source": [
    "with open('twitter/train.txt', 'w') as f:\n",
    "    \n",
    "    for data in train_data:\n",
    "#         print(*data)\n",
    "        for d in data:\n",
    "            f.write(d+\" \")\n",
    "        f.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as f:\n",
    "    \n",
    "    for data in test_data:\n",
    "#         print(*data)\n",
    "        for d in data:\n",
    "            f.write(d)\n",
    "        f.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-81419aba34ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "for i in range(len(data)):\n",
    "    print((data[i]))\n",
    "    if i in test_set:\n",
    "        \n",
    "        print(i,data[i])\n",
    "        test_data.append(data[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444\n",
      "2444\n",
      "114 83636\n",
      "114\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:172: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:60: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:67: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:67: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:365: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:369: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:422: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:424: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000224287CF548>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:114: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PRATEEK\\Downloads\\deep-diffuse-master\\deep-diffuse-master\\glimpse_attention_model.py:128: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from os.path import join\n",
    "\n",
    "import utils\n",
    "from glimpse_attention_model import GlimpseAttentionModel\n",
    "import logging\n",
    "train_len = 5000\n",
    "\n",
    "\n",
    "options = utils.load_params()\n",
    "__processor__ = options['cell_type']\n",
    "# model_type = options['cell_type']\n",
    "handler = logging.FileHandler('{}-{}.log'.format(__processor__, options['dataset_name']), 'w')\n",
    "log = logging.getLogger(__processor__)\n",
    "log.addHandler(handler)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "data_path = join(options['dataset_name'])\n",
    "write_seen_nodes(join(options['dataset_name']), 30)\n",
    "node_index = utils.load_graph(data_path)\n",
    "options['node_size'] = len(node_index)\n",
    "# print(nx.info(G))\n",
    "train_instances, max_diff_train = load_instances(data_path, 'train', node_index, options['seq_len'],\n",
    "                                                       limit=-1)\n",
    "test_instances, max_diff_test = load_instances(data_path, 'test', node_index, options['seq_len'],\n",
    "                                                     limit=-1)\n",
    "options['max_diff'] = max_diff_train\n",
    "# print(len(train_instances), len(test_instances))\n",
    "options['n_train'] = len(train_instances)\n",
    "# print(len(train_instances))\n",
    "train_loader = utils.Loader(train_instances, options)\n",
    "test_loader = utils.Loader(test_instances, options)\n",
    "print(len(train_loader),len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log.info('running glimpse attention model')\n",
    "log.info('using attention:' + str(options['use_attention']))\n",
    "log.info(options)\n",
    "glimpse_ins = GlimpseAttentionModel(options, options['use_attention'], options['n_train'])\n",
    "\n",
    "glimpse_ins.run_model(train_loader, test_loader, options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
